{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f3e4ec8-8ed9-404f-a7de-ddb6a385b6a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca1d3b72-1cc5-46d0-8b6d-8544610a87b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e15646d3-a985-4411-a748-1d43d87f16fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb46d408-be23-4566-827b-1a3d9613fced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Training Images processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b835e020-1585-411f-9ca2-ee89dffe579c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75750 images belonging to 101 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "                    'train', target_size=(64, 64), batch_size = 32, class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4037fd6-c67d-4962-9f6b-e8d4ad4c2b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Testing Images processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a120df07-78e5-4875-b774-70ac3955bb8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25250 images belonging to 101 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = train_datagen.flow_from_directory(\n",
    "                    'test', target_size=(64, 64), batch_size = 32, class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b3438d7-8b20-4698-b7aa-bf2b74ef9082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#builing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9846850c-820a-4a84-b16b-4f9bfee44e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f10d7d9-6b16-4229-bcf7-f6e2bc2a48e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#builing Convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41a27e56-998c-4ff7-b679-adcf37a92c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, activation = 'relu', input_shape=[64, 64, 3]))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33fce813-8ad8-440e-8ffa-baffa1fbf665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, activation = 'relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a54c1953-d3fb-48da-b9f2-b3580849fa91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3ea8aa8-e252-49cd-af95-16cc61baa8c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf1b52b2-ab7d-4d13-be6b-b9c8f4e55123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67a7e8e2-4539-4024-841e-71023c652e96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 64, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c310d64d-6447-41dc-ab16-2f57816326a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 32, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c357c849-14b5-4c37-b1ee-3fa04681b864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 101, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94d7183b-d703-4d5a-8511-4c1d99fc48ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'rmsprop',loss = 'categorical_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b382f23-ab5f-47bb-85e0-99b6099d4f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2368/2368 [==============================] - 480s 202ms/step - loss: 4.5976 - accuracy: 0.0132 - val_loss: 4.5548 - val_accuracy: 0.0238\n",
      "Epoch 2/30\n",
      "2368/2368 [==============================] - 305s 129ms/step - loss: 4.5309 - accuracy: 0.0230 - val_loss: 4.4983 - val_accuracy: 0.0248\n",
      "Epoch 3/30\n",
      "2368/2368 [==============================] - 355s 150ms/step - loss: 4.4577 - accuracy: 0.0334 - val_loss: 4.3896 - val_accuracy: 0.0411\n",
      "Epoch 4/30\n",
      "2368/2368 [==============================] - 321s 135ms/step - loss: 4.3746 - accuracy: 0.0419 - val_loss: 4.2959 - val_accuracy: 0.0514\n",
      "Epoch 5/30\n",
      "2368/2368 [==============================] - 328s 139ms/step - loss: 4.3099 - accuracy: 0.0482 - val_loss: 4.2554 - val_accuracy: 0.0513\n",
      "Epoch 6/30\n",
      "2368/2368 [==============================] - 412s 174ms/step - loss: 4.2629 - accuracy: 0.0521 - val_loss: 4.2075 - val_accuracy: 0.0570\n",
      "Epoch 7/30\n",
      "2368/2368 [==============================] - 339s 143ms/step - loss: 4.2277 - accuracy: 0.0550 - val_loss: 4.1983 - val_accuracy: 0.0580\n",
      "Epoch 8/30\n",
      "2368/2368 [==============================] - 328s 139ms/step - loss: 4.1975 - accuracy: 0.0598 - val_loss: 4.1291 - val_accuracy: 0.0680\n",
      "Epoch 9/30\n",
      "2368/2368 [==============================] - 351s 148ms/step - loss: 4.1723 - accuracy: 0.0645 - val_loss: 4.1239 - val_accuracy: 0.0702\n",
      "Epoch 10/30\n",
      "2368/2368 [==============================] - 364s 154ms/step - loss: 4.1508 - accuracy: 0.0679 - val_loss: 4.0847 - val_accuracy: 0.0769\n",
      "Epoch 11/30\n",
      "2368/2368 [==============================] - 368s 155ms/step - loss: 4.1284 - accuracy: 0.0693 - val_loss: 4.0899 - val_accuracy: 0.0775\n",
      "Epoch 12/30\n",
      "2368/2368 [==============================] - 453s 191ms/step - loss: 4.1126 - accuracy: 0.0726 - val_loss: 4.0770 - val_accuracy: 0.0739\n",
      "Epoch 13/30\n",
      "2368/2368 [==============================] - 376s 159ms/step - loss: 4.0925 - accuracy: 0.0747 - val_loss: 4.0166 - val_accuracy: 0.0832\n",
      "Epoch 14/30\n",
      "2368/2368 [==============================] - 473s 200ms/step - loss: 4.0727 - accuracy: 0.0785 - val_loss: 4.0284 - val_accuracy: 0.0861\n",
      "Epoch 15/30\n",
      "2368/2368 [==============================] - 483s 204ms/step - loss: 4.0589 - accuracy: 0.0814 - val_loss: 4.0417 - val_accuracy: 0.0847\n",
      "Epoch 16/30\n",
      "2368/2368 [==============================] - 459s 194ms/step - loss: 4.0384 - accuracy: 0.0850 - val_loss: 3.9643 - val_accuracy: 0.0953\n",
      "Epoch 17/30\n",
      "2368/2368 [==============================] - 399s 168ms/step - loss: 4.0234 - accuracy: 0.0873 - val_loss: 3.9708 - val_accuracy: 0.0972\n",
      "Epoch 18/30\n",
      "2368/2368 [==============================] - 429s 181ms/step - loss: 4.0097 - accuracy: 0.0890 - val_loss: 3.9826 - val_accuracy: 0.0965\n",
      "Epoch 19/30\n",
      "2368/2368 [==============================] - 484s 205ms/step - loss: 3.9928 - accuracy: 0.0923 - val_loss: 3.9297 - val_accuracy: 0.0986\n",
      "Epoch 20/30\n",
      "2368/2368 [==============================] - 552s 233ms/step - loss: 3.9810 - accuracy: 0.0945 - val_loss: 3.9345 - val_accuracy: 0.1012\n",
      "Epoch 21/30\n",
      "2368/2368 [==============================] - 413s 175ms/step - loss: 3.9662 - accuracy: 0.0968 - val_loss: 3.9550 - val_accuracy: 0.0946\n",
      "Epoch 22/30\n",
      "2368/2368 [==============================] - 373s 158ms/step - loss: 3.9582 - accuracy: 0.0987 - val_loss: 3.9076 - val_accuracy: 0.1065\n",
      "Epoch 23/30\n",
      "2368/2368 [==============================] - 386s 163ms/step - loss: 3.9465 - accuracy: 0.0985 - val_loss: 3.9134 - val_accuracy: 0.1084\n",
      "Epoch 24/30\n",
      "2368/2368 [==============================] - 404s 171ms/step - loss: 3.9326 - accuracy: 0.1020 - val_loss: 3.8950 - val_accuracy: 0.1103\n",
      "Epoch 25/30\n",
      "2368/2368 [==============================] - 481s 203ms/step - loss: 3.9213 - accuracy: 0.1052 - val_loss: 3.8841 - val_accuracy: 0.1087\n",
      "Epoch 26/30\n",
      "2368/2368 [==============================] - 391s 165ms/step - loss: 3.9127 - accuracy: 0.1060 - val_loss: 3.9056 - val_accuracy: 0.1099\n",
      "Epoch 27/30\n",
      "2368/2368 [==============================] - 320s 135ms/step - loss: 3.9026 - accuracy: 0.1056 - val_loss: 3.8581 - val_accuracy: 0.1111\n",
      "Epoch 28/30\n",
      "2368/2368 [==============================] - 349s 147ms/step - loss: 3.8947 - accuracy: 0.1090 - val_loss: 3.8454 - val_accuracy: 0.1187\n",
      "Epoch 29/30\n",
      "2368/2368 [==============================] - 335s 141ms/step - loss: 3.8879 - accuracy: 0.1109 - val_loss: 3.8568 - val_accuracy: 0.1140\n",
      "Epoch 30/30\n",
      "2368/2368 [==============================] - 324s 137ms/step - loss: 3.8736 - accuracy: 0.1131 - val_loss: 3.8297 - val_accuracy: 0.1210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x201d90a4ed0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9423fbc4-eafa-47ee-8c64-0accb065b9d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Preprocess New image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd11ed13-8272-45d2-8a23-afc78e2dae95",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('prediction/samosa.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab32ab-fa71-44c4-bffc-2f78c11098b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c4ff3e7-63db-430a-8b1c-08ee7d8805e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chocolate_cake': 0,\n",
       " 'french_fries': 1,\n",
       " 'ice_cream': 2,\n",
       " 'lasagna': 3,\n",
       " 'mussels': 4,\n",
       " 'nachos': 5,\n",
       " 'oysters': 6,\n",
       " 'pancakes': 7,\n",
       " 'pizza': 8,\n",
       " 'samosa': 9,\n",
       " 'sushi': 10,\n",
       " 'tacos': 11,\n",
       " 'waffles': 12}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e9210c3-2c86-4865-b652-57dbeb7ad363",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd45145f-2682-462a-be80-4921296848a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
